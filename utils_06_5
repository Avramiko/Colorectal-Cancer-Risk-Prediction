# File: utils_06_5.py
import pandas as pd
import numpy as np
import os
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import (
    roc_auc_score, classification_report, confusion_matrix, roc_curve,
    precision_recall_curve, average_precision_score, accuracy_score,
    precision_score, recall_score, f1_score
)

def load_processed_data(data_directory: str):
    train_df = pd.read_csv(os.path.join(data_directory, "train_processed_manual.csv"))
    val_df = pd.read_csv(os.path.join(data_directory, "validation_processed_manual.csv"))
    test_df = pd.read_csv(os.path.join(data_directory, "test_processed_manual.csv"))

    y_train = train_df["CRC_event"]
    X_train = train_df.drop(columns=["CRC_event", "eid"], errors="ignore")
    y_val = val_df["CRC_event"]
    X_val = val_df.drop(columns=["CRC_event", "eid"], errors="ignore")
    y_test = test_df["CRC_event"]
    X_test = test_df.drop(columns=["CRC_event", "eid"], errors="ignore")

    print(f"Train shape: X={X_train.shape}, y={y_train.shape}")
    print(f"Validation shape: X={X_val.shape}, y={y_val.shape}")
    print(f"Test shape: X={X_test.shape}, y={y_test.shape}")

    return X_train, y_train, X_val, y_val, X_test, y_test

def plot_confusion_matrix_professional(y_true, y_pred, model_name, output_dir):
    cm = confusion_matrix(y_true, y_pred)
    cm_percent = cm.astype("float") / cm.sum() * 100

    plt.figure(figsize=(10, 8))
    ax = sns.heatmap(
        cm,
        annot=False,
        cmap="Blues",
        xticklabels=["Predicted: No CRC", "Predicted: CRC"],
        yticklabels=["Actual: No CRC", "Actual: CRC"],
        cbar_kws={"label": "Count"},
        square=True,
    )

    for i in range(2):
        for j in range(2):
            text_color = "white" if cm[i, j] > cm.max() / 2 else "black"
            if i == 0 and j == 0:
                label = f"True Negative\n{cm[i, j]:,}\n({cm_percent[i, j]:.1f}%)"
            elif i == 0 and j == 1:
                label = f"False Positive\n{cm[i, j]:,}\n({cm_percent[i, j]:.1f}%)"
            elif i == 1 and j == 0:
                label = f"False Negative\n{cm[i, j]:,}\n({cm_percent[i, j]:.1f}%)"
            else:
                label = f"True Positive\n{cm[i, j]:,}\n({cm_percent[i, j]:.1f}%)"

            ax.text(
                j + 0.5,
                i + 0.5,
                label,
                ha="center",
                va="center",
                fontsize=12,
                fontweight="bold",
                color=text_color,
                bbox=dict(
                    boxstyle="round,pad=0.3",
                    facecolor="white" if text_color == "black" else "black",
                    alpha=0.8,
                ),
            )

    plt.title(f"Confusion Matrix - {model_name}", fontsize=16, fontweight="bold")
    plt.tight_layout()
    file_path = os.path.join(output_dir, f"confusion_matrix_{model_name}.png")
    plt.savefig(file_path, dpi=300, bbox_inches="tight", facecolor="white")
    plt.close()

def plot_roc_curve(y_true, y_pred_proba, model_name, output_dir):
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    auc_score = roc_auc_score(y_true, y_pred_proba)

    plt.figure(figsize=(8, 8))
    plt.plot(fpr, tpr, lw=2, label=f"ROC curve (AUC = {auc_score:.3f})")
    plt.plot([0, 1], [0, 1], lw=1, linestyle="--", label="Random classifier")
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel("False Positive Rate", fontsize=12)
    plt.ylabel("True Positive Rate", fontsize=12)
    plt.title(f"ROC Curve - {model_name}", fontsize=14, fontweight="bold")
    plt.legend(loc="lower right", fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    file_path = os.path.join(output_dir, f"roc_curve_{model_name}.png")
    plt.savefig(file_path, dpi=300, bbox_inches="tight", facecolor="white")
    plt.close()

def plot_precision_recall_curve(y_true, y_pred_proba, model_name, output_dir):
    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
    avg_precision = average_precision_score(y_true, y_pred_proba)
    baseline = y_true.sum() / len(y_true)

    plt.figure(figsize=(8, 8))
    plt.plot(recall, precision, lw=2, label=f"PR curve (AP = {avg_precision:.3f})")
    plt.axhline(y=baseline, linestyle="--", lw=1, label=f"Random classifier (AP = {baseline:.3f})")
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel("Recall", fontsize=12)
    plt.ylabel("Precision", fontsize=12)
    plt.title(f"Precision-Recall Curve - {model_name}", fontsize=14, fontweight="bold")
    plt.legend(loc="lower left", fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    file_path = os.path.join(output_dir, f"precision_recall_curve_{model_name}.png")
    plt.savefig(file_path, dpi=300, bbox_inches="tight", facecolor="white")
    plt.close()

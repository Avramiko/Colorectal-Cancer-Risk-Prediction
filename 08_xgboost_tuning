# File: 08_xgboost_tuning.py
# Purpose: Find the best hyperparameters for the XGBoost model using RandomizedSearchCV.

import utils_06_5 as utils
import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV
import numpy as np

# --- 1. Load Data ---
X_train, y_train, X_val, y_val, _, _ = utils.load_processed_data("processed_data_manual")

# --- 2. Define Hyperparameter Distribution ---
param_dist = {
    "n_estimators": [100, 200, 300, 400],
    "max_depth": [3, 5, 7, 10],
    "learning_rate": [0.01, 0.05, 0.1, 0.2],
    "subsample": [0.7, 0.8, 0.9, 1.0],
    "colsample_bytree": [0.7, 0.8, 0.9, 1.0],
    "gamma": [0, 0.1, 0.2, 0.5],
    "reg_alpha": [0, 0.01, 0.1, 1],
    "reg_lambda": [0.01, 0.1, 1, 10],
}

# --- 3. Set up Model and Search ---
count_neg = y_train.value_counts()[0]
count_pos = y_train.value_counts()[1]
scale_pos_weight_value = count_neg / count_pos

xgb_model = xgb.XGBClassifier(
    objective="binary:logistic",
    scale_pos_weight=scale_pos_weight_value,
    eval_metric="logloss",
    random_state=42,
    n_jobs=-1,
    tree_method="hist",
    device="cpu"
)

random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_dist,
    n_iter=100,
    scoring="average_precision",
    cv=3,
    random_state=42,
    n_jobs=-1
)

# --- 4. Run Search ---
random_search.fit(X_train, y_train)

# --- 5. Results ---
print(f"Best score (Average Precision): {random_search.best_score_:.4f}")
print("Best parameters:")
print(random_search.best_params_)
